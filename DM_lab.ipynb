{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72d21a3b",
   "metadata": {},
   "source": [
    "# **Analisi Comparativa di Modelli di Embedding per il Clustering di Testi**\n",
    "\n",
    "### **Obiettivo del Notebook**\n",
    "\n",
    "Questo notebook esplora l'efficacia di diversi modelli di embedding per il compito di clustering non supervisionato. L'obiettivo è replicare ed estendere le scoperte dell'articolo scientifico *\"Beyond words: a comparative analysis of LLM embeddings for effective clustering\"*.\n",
    "\n",
    "Partendo da un dataset di notizie non etichettate, confronteremo come diversi modelli linguistici trasformano il testo in vettori numerici (embeddings) e come diversi algoritmi di clustering riescono a raggruppare questi vettori in categorie tematiche coerenti.\n",
    "\n",
    "**Metodologia:**\n",
    "1.  **Dataset:** Utilizzeremo il dataset **BBC News**, composto da 2225 articoli suddivisi in 5 categorie reali (business, entertainment, politics, sport, tech).\n",
    "2.  **Modelli di Embedding:** Confronteremo le performance di due modelli molto noti: `all-MiniLM-L6-v2` (leggero e performante), `stsb-bert-base` (un modello BERT specializzato per la similarità semantica) e `BLOOMZ-3B` (modello general-purpose piu' grande degli altri). \n",
    "3.  **Algoritmi di Clustering:** Implementeremo un framework flessibile per testare diversi algoritmi. Sono stati scelti, `k-means` (variante ++), `CAEclust`, `Deep k-means`.\n",
    "4.  **Valutazione:** Misureremo la qualità del clustering utilizzando due metriche standard:\n",
    "    *   **Accuracy (ACC):** Metrica di precisione relativa al \"ground truth\".\n",
    "    *   **Adjusted Rand Index (ARI):** Misura la somiglianza tra i cluster predetti e le etichette reali, correggendo per il caso (valori vicini a 1.0 sono migliori).\n",
    "    *   **Normalized Mutual Information (NMI):** Valuta la quantità di informazione condivisa tra i cluster predetti e le etichette reali (valori vicini a 1.0 sono migliori)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8809969d",
   "metadata": {},
   "source": [
    "### **1. Setup dell'Ambiente e Configurazione**\n",
    "In questa prima cella di codice, importiamo tutte le librerie necessarie e definiamo le variabili di configurazione globali. Centralizzare la configurazione qui rende il notebook più pulito e facile da adattare per futuri esperimenti.\n",
    "\n",
    "> **Nota:** Assicurati di aver installato tutte le librerie richieste nel tuo ambiente `conda` o `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8ba5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from itertools import permutations\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "\n",
    "# --- Librerie per Machine Learning e NLP ---\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from sklearn.cluster import KMeans, SpectralClustering\n",
    "    from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"ERRORE: Libreria mancante -> {e.name}\")\n",
    "    print(\"Assicurati di aver installato 'scikit-learn' e 'sentence-transformers'.\")\n",
    "\n",
    "# Ignora avvisi non critici per una migliore leggibilità dell'output\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# --- CONFIGURAZIONE PRINCIPALE ---\n",
    "\n",
    "# Percorso del dataset\n",
    "DATASET_PATH = \"./dataset\"\n",
    "\n",
    "# Modelli di embedding\n",
    "MODELS_TO_TEST = {\n",
    "    'MiniLM': 'all-MiniLM-L6-v2',\n",
    "    'BERT_STS': 'stsb-bert-base', \n",
    "    'BLOOMZ_3B': 'bigscience/bloomz-3b' # Identificatore ufficiale su Hugging Face\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee65195",
   "metadata": {},
   "source": [
    "### **2. Funzioni di Supporto**\n",
    "Per mantenere il codice modulare e riutilizzabile, definiamo due funzioni principali:\n",
    "1.  `load_data_from_folders`: Legge i file `.txt` dalle sottocartelle del dataset e li carica in memoria insieme alle loro etichette reali (il nome della cartella).\n",
    "2.  `get_or_create_embeddings`: Una funzione cruciale che gestisce la generazione degli embedding. Implementa un meccanismo di **caching**: se gli embedding per un modello sono già stati calcolati e salvati su un file `.npy`, li carica direttamente; altrimenti, li genera da zero e li salva per le esecuzioni future. Questo fa risparmiare moltissimo tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de061820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data_from_folders(path):\n",
    "    \"\"\"Carica i testi e le etichette, applicando il lowercase.\"\"\"\n",
    "    texts, labels = [], []\n",
    "    if not os.path.exists(path): return None, None\n",
    "    categories = sorted([d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))])\n",
    "    if not categories: return None, None\n",
    "    print(f\"Trovate {len(categories)} categorie: {categories}\")\n",
    "    for category in categories:\n",
    "        category_path = os.path.join(path, category)\n",
    "        for filename in sorted(os.listdir(category_path)):\n",
    "            if filename.endswith('.txt'):\n",
    "                with open(os.path.join(path, category, filename), 'r', encoding='latin-1') as f:\n",
    "                    texts.append(f.read().lower())\n",
    "                    labels.append(category)\n",
    "    print(f\"Caricati e pre-processati (lowercase) {len(texts)} documenti.\")\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "\n",
    "def get_embeddings_with_chunking(model, texts, chunk_size=512, overlap=64):\n",
    "    \"\"\"\n",
    "    Genera embedding per documenti lunghi dividendoli in chunk e facendo la media.\n",
    "    Converte esplicitamente il tipo di dati prima di passare a NumPy.\n",
    "    \"\"\"\n",
    "    final_embeddings = []\n",
    "    for doc in tqdm(texts, desc=\"Processing documents with chunking\"):\n",
    "        if not doc.strip():\n",
    "            final_embeddings.append(np.zeros(model.get_sentence_embedding_dimension()))\n",
    "            continue\n",
    "        tokens = model.tokenizer.encode(doc, add_special_tokens=False)\n",
    "        chunk_embeddings = []\n",
    "        for i in range(0, len(tokens), chunk_size - overlap):\n",
    "            chunk_tokens = tokens[i:i + chunk_size]\n",
    "            if not chunk_tokens: continue\n",
    "            chunk_text = model.tokenizer.decode(chunk_tokens)\n",
    "            if not chunk_text.strip(): continue\n",
    "            # Calcoliamo l'embedding del chunk (sarà in bfloat16)\n",
    "            chunk_embedding = model.encode(chunk_text, convert_to_tensor=True, show_progress_bar=False)\n",
    "            chunk_embeddings.append(chunk_embedding)\n",
    "        \n",
    "        if chunk_embeddings:\n",
    "            # Calcoliamo la media (il risultato è ancora in bfloat16)\n",
    "            mean_embedding = torch.mean(torch.stack(chunk_embeddings), dim=0)\n",
    "            \n",
    "            # Convertiamo il tensore in float32 PRIMA di passarlo a cpu() e numpy()\n",
    "            final_embeddings.append(mean_embedding.float().cpu().numpy())\n",
    "        else:\n",
    "            final_embeddings.append(np.zeros(model.get_sentence_embedding_dimension()))\n",
    "\n",
    "    return np.array(final_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_or_create_embeddings(texts, model_name, friendly_name):\n",
    "    \"\"\"Genera gli embedding con una logica di device pulita e robusta, usando bfloat16 per i modelli grandi.\"\"\"\n",
    "    embedding_file = f\"bbc_embeddings_{friendly_name}_lowercase.npy\"\n",
    "    if os.path.exists(embedding_file):\n",
    "        print(f\"Caricamento degli embedding per '{friendly_name}' dal file locale: {embedding_file}\")\n",
    "        return np.load(embedding_file)\n",
    "    else:\n",
    "        print(f\"Generazione degli embedding per '{friendly_name}' con il modello '{model_name}'...\")\n",
    "        \n",
    "        model_kwargs = {}\n",
    "        st_kwargs = {}\n",
    "        needs_chunking = False\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        if 'bloomz' in model_name.lower():\n",
    "             print(\"Applicazione ottimizzazioni per modello BLOOMZ (bfloat16 e device_map)...\")\n",
    "             model_kwargs['torch_dtype'] = torch.bfloat16 \n",
    "             model_kwargs['device_map'] = 'auto'\n",
    "             needs_chunking = True\n",
    "        else:\n",
    "             st_kwargs['device'] = device\n",
    "\n",
    "        if 'device_map' in model_kwargs:\n",
    "            st_kwargs = {}\n",
    "\n",
    "        model = SentenceTransformer(model_name, model_kwargs=model_kwargs, **st_kwargs)\n",
    "        \n",
    "        if needs_chunking:\n",
    "            embeddings = get_embeddings_with_chunking(model, texts)\n",
    "        else:\n",
    "            batch_size = 32\n",
    "            print(f\"Inizio encoding con batch_size = {batch_size}...\")\n",
    "            embeddings = model.encode(texts, show_progress_bar=True, batch_size=batch_size)\n",
    "        \n",
    "        # Aggiungiamo un controllo per i valori NaN/inf prima di salvare\n",
    "        if np.any(np.isnan(embeddings)) or np.any(np.isinf(embeddings)):\n",
    "            print(\"ATTENZIONE: Trovati valori NaN o Inf negli embedding. Rimuovendoli...\")\n",
    "            # Sostituisce NaN e Inf con zero\n",
    "            embeddings = np.nan_to_num(embeddings, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Pulizia della memoria GPU completata.\")\n",
    "        \n",
    "        print(f\"Salvataggio degli embedding su file: {embedding_file}\")\n",
    "        np.save(embedding_file, embeddings)\n",
    "        return np.array(embeddings)\n",
    "    \n",
    "def calculate_accuracy(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calcola l'accuratezza del clustering (ACC) trovando la migliore mappatura\n",
    "    tra le etichette predette e quelle reali usando le permutazioni.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- CORREZIONE: Converti gli input in array NumPy per confronti robusti ---\n",
    "    true_labels = np.asarray(true_labels)\n",
    "    predicted_labels = np.asarray(predicted_labels)\n",
    "    \n",
    "    # 1. Trova le etichette uniche\n",
    "    true_unique_labels = np.unique(true_labels)\n",
    "    pred_unique_labels = np.unique(predicted_labels)\n",
    "    \n",
    "    # Gestisce il caso in cui il clustering trovi un numero diverso di cluster\n",
    "    if len(true_unique_labels) != len(pred_unique_labels):\n",
    "        print(f\"ATTENZIONE: Il numero di cluster reali ({len(true_unique_labels)}) è diverso dal numero di cluster predetti ({len(pred_unique_labels)}). L'accuratezza potrebbe non essere significativa.\")\n",
    "        # In questo caso, la mappatura uno-a-uno non è possibile, restituiamo 0.0\n",
    "        return 0.0\n",
    "\n",
    "    n_clusters = len(pred_unique_labels)\n",
    "    \n",
    "    # 2. Crea la matrice di contingenza\n",
    "    contingency = np.zeros((n_clusters, n_clusters), dtype=np.int64)\n",
    "    for true_idx, true_label in enumerate(true_unique_labels):\n",
    "        for pred_idx, pred_label in enumerate(pred_unique_labels):\n",
    "            contingency[true_idx, pred_idx] = np.sum((true_labels == true_label) & (predicted_labels == pred_label))\n",
    "\n",
    "    # 3. Itera su tutte le possibili mappature (permutazioni)\n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for perm in permutations(range(n_clusters)):\n",
    "        # `perm` mappa l'indice della riga (etichetta reale) all'indice della colonna (etichetta predetta)\n",
    "        current_correct_count = contingency[range(n_clusters), perm].sum()\n",
    "        \n",
    "        accuracy = current_correct_count / np.sum(contingency)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            \n",
    "    return best_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb0d5ab",
   "metadata": {},
   "source": [
    "### **3. Definizione degli Algoritmi di Clustering**\n",
    "Questa cella rende il nostro framework di test estremamente flessibile. La funzione `get_clustering_algorithms` restituisce un dizionario di algoritmi di `scikit-learn` pronti per essere utilizzati.\n",
    "\n",
    "**Per testare un nuovo algoritmo di clustering, è sufficiente:**\n",
    "1.  Importarlo nella Cella 1 (Snippet 3).\n",
    "2.  Aggiungere una nuova voce al dizionario `algorithms` in questa cella.\n",
    "\n",
    "Iniziamo con `K-Means++`, dove \"++\" si riferisce a un metodo di inizializzazione intelligente che migliora la stabilità e la qualità dei cluster finali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff62e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Definizione dell'Autoencoder Profondo  ---\n",
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Un autoencoder più profondo con regularizzazione Dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, bottleneck_dim=64, dropout_rate=0.2):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, bottleneck_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck_dim, 128), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 256), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, input_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "# --- 2. Wrapper per l'algoritmo CAEclust ---\n",
    "class CAEclustWrapper:\n",
    "    def __init__(self, n_clusters, n_autoencoders=5, bottleneck_dim=128, epochs=100, dropout_rate=0.2):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_autoencoders = n_autoencoders\n",
    "        self.bottleneck_dim = bottleneck_dim\n",
    "        self.epochs = epochs\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"CAEclust (PyTorch) inizializzato: {self.n_autoencoders} DAEs, bottleneck a {self.bottleneck_dim}, dropout {self.dropout_rate}, {self.epochs} epoche. Device: {self.device}\")\n",
    "\n",
    "    def fit_predict(self, embeddings):\n",
    "        scaler = StandardScaler()\n",
    "        scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "        input_dim = scaled_embeddings.shape[1]\n",
    "        embeddings_tensor = torch.FloatTensor(scaled_embeddings).to(self.device)\n",
    "        trained_encoders = []\n",
    "\n",
    "        print(f\"\\nInizio addestramento di {self.n_autoencoders} Denoising Autoencoder...\")\n",
    "        for i in range(self.n_autoencoders):\n",
    "            print(f\"  Training DAE {i+1}/{self.n_autoencoders}...\")\n",
    "            model = Autoencoder(input_dim, self.bottleneck_dim, self.dropout_rate).to(self.device)\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "            noise_factor = 0.05\n",
    "            noisy_embeddings_tensor = embeddings_tensor + noise_factor * torch.randn_like(embeddings_tensor)\n",
    "            dataset = TensorDataset(noisy_embeddings_tensor, embeddings_tensor)\n",
    "            data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "            model.train()\n",
    "            for epoch in range(self.epochs):\n",
    "                for noisy_inputs, clean_targets in data_loader:\n",
    "                    outputs = model(noisy_inputs)\n",
    "                    loss = criterion(outputs, clean_targets)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            trained_encoders.append(model.encoder)\n",
    "\n",
    "        consensus_features_tensor = torch.FloatTensor(scaled_embeddings).to(self.device)\n",
    "        consensus_representations = []\n",
    "        with torch.no_grad():\n",
    "            for encoder in trained_encoders:\n",
    "                encoder.eval()\n",
    "                encoded_data = encoder(consensus_features_tensor).cpu().numpy()\n",
    "                consensus_representations.append(encoded_data)\n",
    "        \n",
    "        consensus_features = np.concatenate(consensus_representations, axis=1)\n",
    "        consensus_features = StandardScaler().fit_transform(consensus_features)\n",
    "        print(f\"Nuova forma delle feature di consenso: {consensus_features.shape}\")\n",
    "        \n",
    "        print(\"Applicazione di Spectral Clustering...\")\n",
    "        clusterer = SpectralClustering(n_clusters=self.n_clusters, random_state=42, affinity='nearest_neighbors', n_jobs=-1)\n",
    "        labels = clusterer.fit_predict(consensus_features)\n",
    "        return labels\n",
    "\n",
    "# --- 3. Wrapper per l'algoritmo Deep K-Means ---\n",
    "class DeepKMeansWrapper:\n",
    "    def __init__(self, n_clusters, bottleneck_dim=64, epochs=100, gamma=0.1):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.bottleneck_dim = bottleneck_dim\n",
    "        self.epochs = epochs\n",
    "        self.gamma = gamma\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Deep K-Means (PyTorch) inizializzato: bottleneck a {self.bottleneck_dim}, {self.epochs} epoche, gamma={self.gamma}. Device: {self.device}\")\n",
    "\n",
    "    def _calculate_clustering_loss(self, latent_vectors, centroids):\n",
    "        distances = torch.sum((latent_vectors.unsqueeze(1) - centroids) ** 2, dim=2)\n",
    "        min_distances, _ = torch.min(distances, dim=1)\n",
    "        return torch.mean(min_distances)\n",
    "\n",
    "    def fit_predict(self, embeddings):\n",
    "        scaler = StandardScaler()\n",
    "        scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "        input_dim = scaled_embeddings.shape[1]\n",
    "        embeddings_tensor = torch.FloatTensor(scaled_embeddings).to(self.device)\n",
    "        data_loader = DataLoader(TensorDataset(embeddings_tensor), batch_size=64, shuffle=True)\n",
    "\n",
    "        model = Autoencoder(input_dim, self.bottleneck_dim).to(self.device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "        reconstruction_criterion = nn.MSELoss()\n",
    "\n",
    "        # Pre-training\n",
    "        print(\"\\nInizio pre-training dell'Autoencoder (20 epoche)...\")\n",
    "        for epoch in range(20):\n",
    "            for batch in data_loader:\n",
    "                inputs, = batch\n",
    "                outputs = model(inputs)\n",
    "                loss = reconstruction_criterion(outputs, inputs)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        print(\"Inizializzazione dei centroidi con K-Means...\")\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            initial_latent_space = model.encoder(embeddings_tensor).cpu().numpy()\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, init='k-means++', n_init=10, random_state=42)\n",
    "        kmeans.fit(initial_latent_space)\n",
    "        centroids = torch.FloatTensor(kmeans.cluster_centers_).to(self.device)\n",
    "\n",
    "        # Training Congiunto (DKM)\n",
    "        print(\"Inizio addestramento congiunto (Deep K-Means)...\")\n",
    "        model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch in data_loader:\n",
    "                inputs, = batch\n",
    "                latent_vectors = model.encoder(inputs)\n",
    "                reconstructed_vectors = model.decoder(latent_vectors)\n",
    "                recon_loss = reconstruction_criterion(reconstructed_vectors, inputs)\n",
    "                clust_loss = self._calculate_clustering_loss(latent_vectors, centroids)\n",
    "                total_loss = recon_loss + self.gamma * clust_loss\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Assegnazione finale\n",
    "        print(\"Assegnazione finale ai cluster...\")\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            final_latent_space = model.encoder(embeddings_tensor).cpu().numpy()\n",
    "        \n",
    "        final_kmeans = KMeans(n_clusters=self.n_clusters, init=centroids.cpu().numpy(), n_init=1, random_state=42)\n",
    "        labels = final_kmeans.fit_predict(final_latent_space)\n",
    "        return labels\n",
    "\n",
    "# --- 4. Funzione per ottenere tutti gli algoritmi ---\n",
    "def get_clustering_algorithms(n_clusters):\n",
    "    \"\"\"\n",
    "    Restituisce un dizionario di algoritmi di clustering, inclusi i metodi deep.\n",
    "    \"\"\"\n",
    "    algorithms = {\n",
    "        'K-Means++': KMeans(n_clusters=n_clusters, init='k-means++', random_state=42, n_init=10),\n",
    "        'CAEclust (Deep)': CAEclustWrapper(n_clusters=n_clusters),\n",
    "        'Deep K-Means': DeepKMeansWrapper(n_clusters=n_clusters)\n",
    "    }\n",
    "    return algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb1eb26",
   "metadata": {},
   "source": [
    "### **4. Esperimento 1 - Modello `all-MiniLM-L6-v2`**\n",
    "\n",
    "Iniziamo il nostro primo esperimento con **MiniLM**. Questo modello è noto per il suo eccellente equilibrio tra velocità, dimensioni ridotte e alta performance, rendendolo una baseline molto forte.\n",
    "\n",
    "Il codice nella cella successiva eseguirà i seguenti passaggi:\n",
    "1.  Carica i dati testuali (se non già presenti in memoria).\n",
    "2.  Ottiene gli embedding per il modello MiniLM.\n",
    "3.  Esegue un ciclo su tutti gli algoritmi definiti nella Cella 3.\n",
    "4.  Calcola e stampa le metriche di performance ARI e NMI per ogni algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58dd61d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trovate 5 categorie: ['.git', 'business', 'entertainment', 'politics', 'sport']\n",
      "Caricati e pre-processati (lowercase) 1824 documenti.\n",
      "\n",
      "======================================================================\n",
      "===== INIZIO TEST MODELLO: MiniLM =====\n",
      "Caricamento degli embedding per 'MiniLM' dal file locale: bbc_embeddings_MiniLM_lowercase.npy\n",
      "Forma della matrice di embedding: (1824, 384)\n",
      "CAEclust (PyTorch) inizializzato: 5 DAEs, bottleneck a 128, dropout 0.2, 100 epoche. Device: cuda\n",
      "Deep K-Means (PyTorch) inizializzato: bottleneck a 64, 100 epoche, gamma=0.1. Device: cuda\n",
      "\n",
      "--- Applicazione dell'algoritmo: K-Means++ ---\n",
      "Risultati per K-Means++: Accuracy (ACC): 0.9731, ARI = 0.9308, NMI = 0.9000\n",
      "\n",
      "--- Applicazione dell'algoritmo: CAEclust (Deep) ---\n",
      "\n",
      "Inizio addestramento di 5 Denoising Autoencoder...\n",
      "  Training DAE 1/5...\n",
      "  Training DAE 2/5...\n",
      "  Training DAE 3/5...\n",
      "  Training DAE 4/5...\n",
      "  Training DAE 5/5...\n",
      "Nuova forma delle feature di consenso: (1824, 640)\n",
      "Applicazione di Spectral Clustering...\n",
      "Risultati per CAEclust (Deep): Accuracy (ACC): 0.6634, ARI = 0.5887, NMI = 0.7376\n",
      "\n",
      "--- Applicazione dell'algoritmo: Deep K-Means ---\n",
      "\n",
      "Inizio pre-training dell'Autoencoder (20 epoche)...\n",
      "Inizializzazione dei centroidi con K-Means...\n",
      "Inizio addestramento congiunto (Deep K-Means)...\n",
      "Assegnazione finale ai cluster...\n",
      "Risultati per Deep K-Means: Accuracy (ACC): 0.9731, ARI = 0.9301, NMI = 0.9076\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Caricamento dei dati (da eseguire solo una volta se non già fatto)\n",
    "if 'texts' not in locals():\n",
    "    texts, true_labels = load_data_from_folders(DATASET_PATH)\n",
    "    n_clusters = len(set(true_labels))\n",
    "\n",
    "# 2. Test per MiniLM\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"===== INIZIO TEST MODELLO: MiniLM =====\")\n",
    "minilm_embeddings = get_or_create_embeddings(texts, MODELS_TO_TEST['MiniLM'], 'MiniLM')\n",
    "print(f\"Forma della matrice di embedding: {minilm_embeddings.shape}\")\n",
    "\n",
    "# 3. Applica tutti gli algoritmi di clustering definiti\n",
    "clustering_algorithms_to_run = get_clustering_algorithms(n_clusters)\n",
    "for algo_name, algorithm in clustering_algorithms_to_run.items():\n",
    "    print(f\"\\n--- Applicazione dell'algoritmo: {algo_name} ---\")\n",
    "    predicted_labels = algorithm.fit_predict(minilm_embeddings)\n",
    "    \n",
    "    ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    nmi = normalized_mutual_info_score(true_labels, predicted_labels)\n",
    "    acc = calculate_accuracy(true_labels, predicted_labels)\n",
    "    \n",
    "    print(f\"Risultati per {algo_name}: Accuracy (ACC): {acc:.4f}, ARI = {ari:.4f}, NMI = {nmi:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802b0baa",
   "metadata": {},
   "source": [
    "### **5. Esperimento 2 - Modello `stsb-bert-base`**\n",
    "Ora ripetiamo l'esperimento utilizzando un modello **BERT** che è stato specificamente addestrato (fine-tuned) per compiti di **Semantic Textual Similarity (STS)**. A differenza di un BERT generico, questo modello è ottimizzato per produrre embedding in cui la distanza tra i vettori riflette fedelmente la similarità semantica dei testi.\n",
    "\n",
    "Questo confronto è cruciale per capire quanto la specializzazione di un modello influenzi la sua efficacia in un compito di clustering tematico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42a13c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Esperimento 2: BERT (STS) ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"===== INIZIO TEST MODELLO: BERT (STS) =====\")\n",
    "\n",
    "# 1. Carica i dati freschi per questo esperimento\n",
    "texts, true_labels = load_data_from_folders(DATASET_PATH)\n",
    "if texts is None:\n",
    "    print(\"Caricamento dati fallito. Salto dell'esperimento.\")\n",
    "else:\n",
    "    n_clusters = len(set(true_labels))\n",
    "\n",
    "    # 2. Ottieni gli embedding per BERT (STS)\n",
    "    bert_embeddings = get_or_create_embeddings(texts, MODELS_TO_TEST['BERT_STS'], 'BERT_STS')\n",
    "    print(f\"Forma della matrice di embedding: {bert_embeddings.shape}\")\n",
    "\n",
    "    # 3. Applica tutti gli algoritmi di clustering definiti\n",
    "    clustering_algorithms_to_run = get_clustering_algorithms(n_clusters)\n",
    "    for algo_name, algorithm in clustering_algorithms_to_run.items():\n",
    "        print(f\"\\n--- Applicazione dell'algoritmo: {algo_name} ---\")\n",
    "        predicted_labels = algorithm.fit_predict(bert_embeddings)\n",
    "        \n",
    "        # Verifica di coerenza prima di calcolare i punteggi\n",
    "        if len(true_labels) == len(predicted_labels):\n",
    "            ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "            nmi = normalized_mutual_info_score(true_labels, predicted_labels)\n",
    "            acc = calculate_accuracy(true_labels, predicted_labels)\n",
    "            print(f\"Risultati per {algo_name}: Accuracy (ACC): {acc:.4f}, ARI = {ari:.4f}, NMI = {nmi:.4f}\")\n",
    "        else:\n",
    "            print(f\"ERRORE: Incoerenza nelle dimensioni! Etichette reali: {len(true_labels)}, Etichette predette: {len(predicted_labels)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6873d3e",
   "metadata": {},
   "source": [
    "### **6. Esperimento 3 - Modello `BLOOMZ-3B`**\n",
    "\n",
    "\n",
    "Dopo aver analizzato i modelli specializzati, è il momento di testare un modello di una classe completamente diversa: **`bigscience/bloomz-3b`**. Con i suoi 3 miliardi di parametri, questo modello rappresenta un approccio \"generalista\" e su larga scala.\n",
    "\n",
    "Poiché BLOOMZ non è stato primariamente progettato per produrre embedding di documenti, utilizzeremo la strategia standard del **mean pooling** per estrarre un singolo vettore da tutti i suoi output.\n",
    "\n",
    "**Nota Tecnica:** L'esecuzione di questo modello ha richiesto ottimizzazioni significative a causa delle sue dimensioni. Abbiamo dovuto implementare il caricamento in mezza precisione (`bfloat16`), la distribuzione automatica del modello tra CPU e GPU (`device_map=\"auto\"`) e una strategia di **chunking** per gestire i documenti lunghi senza esaurire la memoria della GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ccefc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"===== INIZIO TEST MODELLO: BLOOMZ-3B =====\")\n",
    "\n",
    "# 1. Carica i dati freschi\n",
    "texts, true_labels = load_data_from_folders(DATASET_PATH)\n",
    "n_clusters = len(set(true_labels))\n",
    "\n",
    "# 2. Ottieni gli embedding per BLOOMZ-3B\n",
    "bloomz_embeddings = get_or_create_embeddings(texts, MODELS_TO_TEST['BLOOMZ_3B'], 'BLOOMZ_3B')\n",
    "print(f\"Forma della matrice di embedding: {bloomz_embeddings.shape}\")\n",
    "\n",
    "# 3. Applica tutti gli algoritmi di clustering definiti\n",
    "clustering_algorithms_to_run = get_clustering_algorithms(n_clusters)\n",
    "for algo_name, algorithm in clustering_algorithms_to_run.items():\n",
    "    print(f\"\\n--- Applicazione dell'algoritmo: {algo_name} ---\")\n",
    "    predicted_labels = algorithm.fit_predict(bloomz_embeddings)\n",
    "    \n",
    "    ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    nmi = normalized_mutual_info_score(true_labels, predicted_labels)\n",
    "    acc = calculate_accuracy(true_labels, predicted_labels)\n",
    "    \n",
    "    print(f\"Risultati per {algo_name}: Accuracy (ACC): {acc:.4f}, ARI = {ari:.4f}, NMI = {nmi:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
